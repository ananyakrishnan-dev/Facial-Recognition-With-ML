# -*- coding: utf-8 -*-
"""Testproject1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wsXhdPv91Q_q9NwgMJa24_gFxFpWM0Zc
"""

#install a compatible version of imgaug
!pip uninstall -y imgaug && pip uninstall -y albumentations && pip install git+https://github.com/aleju/imgaug.git
!pip install face_recognition

#import opencv and cv2
!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python
import cv2

#more imports for face detection
import imutils
import numpy as np
from google.colab.patches import cv2_imshow
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode

#not necessary but part of tutorial
import time
time.asctime()

#import imshow so image can be shown here after being read
from google.colab.patches import cv2_imshow

from google.colab import drive
drive.mount('/content/drive')

#read training image and resize it to have a maximum width of 400 pixels
training_img=cv2.imread("/content/drive/MyDrive/Screen Shot 2021-07-19 at 12.27.53 PM.png")
training_img = imutils.resize(training_img, width=400)
(h, w) = training_img.shape[:2]
print(w,h)
cv2_imshow(training_img)

#Download the pre-trained face detection model, consisting of two files:The network definition (deploy.prototxt) and The learned weights (res10_300x300_ssd_iter_140000.caffemodel)

!wget -N https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt
!wget -N https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel

#Load the pre-trained face detection network model from diskprint("[INFO] loading model...")
prototxt = 'deploy.prototxt'
model = 'res10_300x300_ssd_iter_140000.caffemodel'
net = cv2.dnn.readNetFromCaffe(prototxt, model)

#Use the dnn.blobFromImage function to construct an input blob by resizing the image to a fixed 300x300 pixels and then normalizing it.
training_img = imutils.resize(training_img, width=400)
blob = cv2.dnn.blobFromImage(cv2.resize(training_img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))

#Pass the blob through the neural network and obtain the detections and predictions
print("[INFO] computing object detections...")
net.setInput(blob)
detections = net.forward()

for i in range(0, detections.shape[2]):

	# extract the confidence (i.e., probability) associated with the prediction
	confidence = detections[0, 0, i, 2]

	# filter out weak detections by ensuring the `confidence` is
	# greater than the minimum confidence threshold
	if confidence > 0.5:
		# compute the (x, y)-coordinates of the bounding box for the object
		box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
		(startX, startY, endX, endY) = box.astype("int")
		# draw the bounding box of the face along with the associated probability
		text = "{:.2f}%".format(confidence * 100)
		y = startY - 10 if startY - 10 > 10 else startY + 10
		cv2.rectangle(training_img, (startX, startY), (endX, endY), (0, 0, 255), 2)
		cv2.putText(training_img, text, (startX, y),
			cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)

#Show the resulting image
cv2_imshow(training_img)

#image = cv2.imread(image_file, cv2.IMREAD_UNCHANGED)
final_img=cv2.imread("/content/drive/MyDrive/Screen Shot 2021-07-19 at 12.42.42 PM.png")
# resize it to have a maximum width of 400 pixels
final_img = imutils.resize(final_img, width=400)
(h, w) = final_img.shape[:2]
print(w,h)
cv2_imshow(final_img)

!wget -N https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt
!wget -N https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel

print("[INFO] loading model...")
prototxt = 'deploy.prototxt'
model = 'res10_300x300_ssd_iter_140000.caffemodel'
net = cv2.dnn.readNetFromCaffe(prototxt, model)

final_img = imutils.resize(final_img, width=400)
blob = cv2.dnn.blobFromImage(cv2.resize(final_img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))

print("[INFO] computing object detections...")
net.setInput(blob)
detections = net.forward()

print(detections.shape[2])

for i in range(0, detections.shape[2]):

	# extract the confidence (i.e., probability) associated with the prediction
	confidence = detections[0, 0, i, 2]

	# filter out weak detections by ensuring the `confidence` is
	# greater than the minimum confidence threshold
	if confidence > 0.5:
		# compute the (x, y)-coordinates of the bounding box for the object
		box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
		(startX, startY, endX, endY) = box.astype("int")
		# draw the bounding box of the face along with the associated probability
		text = "{:.2f}%".format(confidence * 100)
		y = startY - 10 if startY - 10 > 10 else startY + 10
		cv2.rectangle(final_img, (startX, startY), (endX, endY), (0, 0, 255), 2)
		cv2.putText(final_img, text, (startX, y),
			cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)

cv2_imshow(final_img)

fruits = ["apple", "banana", "pear"]
for index, i in enumerate(fruits):
  print(f"{i} is at index {index}")

import face_recognition
import cv2
import os
from google.colab.patches import cv2_imshow

def read_img(path):
  img = cv2.imread(path)
  (h,w) = img.shape[:2]
  width = 500
  ratio = width / float(w)
  height = int(h * ratio)
  return cv2.resize(img,(width,height))

# compare_faces(
#    [img_knownEncoding], # parameter one
#    imgunknownEncoding # parameter two
#    0.3,
# )
  
img_known = read_img("/content/drive/MyDrive/Screen Shot 2021-07-19 at 12.27.53 PM.png")
img_knownEncoding = face_recognition.face_encodings(img_known)[0]

img_unknown = read_img("/content/drive/MyDrive/Screen Shot 2021-07-19 at 12.42.42 PM.png")
img_unknownEncoding = face_recognition.face_encodings(img_unknown)

for index, i in enumerate(img_unknownEncoding):
  
  results = face_recognition.compare_faces([img_knownEncoding], i)
  if results[0] == True :
    #res = [i for i, val in enumerate(results) if val] 
    (top,right,bottom,left) = face_recognition.face_locations(img_unknown)[index]
    cv2.rectangle(img_unknown,(left,top),(right,bottom),(255,255,100),2)
    
cv2_imshow(img_unknown)

print(results)

print(img_unknownEncoding)